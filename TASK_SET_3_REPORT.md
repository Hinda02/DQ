# TASK SET 3: Sampling and FD Hypotheses - Validation Report

## Executive Summary

**Objective**: Validate LLM-proposed functional dependencies from sample data against full datasets to demonstrate how sampling affects FD discovery.

**Key Finding**: LLMs generate **massively overfitted** FDs from small samples, with **67% being non-minimal** and **15% being false positives**.

---

## What Task Set 3 Asked

### Requirements:
1. **Sample the dataset**: Create random, stratified, or biased samples (max 50 rows)
2. **Show samples to LLM**: Ask for likely FDs (without showing full dataset/FD list)
3. **Validate FDs**: Check if LLM-proposed FDs hold on:
   - The sample (should hold by construction)
   - The full dataset (may or may not hold)
4. **Report violations** and approximate validity
5. **Answer questions**:
   - Which FDs are false positives?
   - Which are not minimal?
   - Which are misleading but "look right"?

**Key Principle**: *"Sampling creates hypotheses, not truth."*

---

## What Was Provided

The repository includes:
- **samples/** folder with 20 sample files (random, stratified, biased sampling)
- **FDs_summary.xlsx** containing LLM-proposed FDs for each sample
- Samples range from 33-51 rows (max ~50 as per assignment)

---

## Methodology

### 1. Parsed LLM Proposals
Extracted FDs from Excel file containing LLM responses to samples.

### 2. Validation Process
For each LLM-proposed FD:
- âœ… **Sample validation**: Check if holds on the sample data
- âœ… **Full dataset validation**: Check if holds on complete dataset
- âœ… **Minimality check**: Test if any LHS attribute is redundant
- âœ… **Classification**: Categorize as false positive, non-minimal, overfitted, or valid

### 3. Automated Testing
Created `validate_llm_fds.py` to systematically check all 20+ FDs.

---

## Results: LLM-Proposed FDs

### Summary Statistics

| Metric | Count | Percentage |
|--------|-------|------------|
| **Total LLM-proposed FDs** | 20 | 100% |
| **False positives** | 3 | 15% |
| **Non-minimal FDs** | 13 | 65% |
| **Overfitted (â‰¥6 attrs on LHS)** | 13 | 65% |
| **Valid and minimal** | 4 | 20% |

---

## Detailed Analysis by Dataset

### 1. IRIS (4 samples, 2 FDs)

**LLM Proposal**: `(1,2,3,4) â†’ 5` (all 4 measurements â†’ species)

| Sample Type | Holds on Sample | Holds on Full | Minimal? | Classification |
|-------------|-----------------|---------------|----------|----------------|
| Random | âœ… Yes | âœ… Yes | âŒ No | Non-minimal |
| Biased | âœ… Yes | âœ… Yes | âŒ No | Non-minimal |

**Redundant columns**: ALL (1,2,3,4)

**Analysis**:
- LLM correctly identified that measurements determine species
- BUT proposed using ALL 4 attributes when fewer would suffice
- Task Set 1 showed that only 3 attributes are needed
- **Verdict**: True relationship but **non-minimal** (overfitted)

---

### 2. ABALONE (3 samples, 1 FD)

**LLM Proposal**: `(1,2,3,4,5,6,7,8) â†’ 9` (all 8 measurements â†’ rings/age)

| Sample Type | Holds on Sample | Holds on Full | Minimal? | Classification |
|-------------|-----------------|---------------|----------|----------------|
| Random | âœ… Yes | âœ… Yes | âŒ No | Non-minimal, Overfitted |
| Stratified | âœ… Yes | âœ… Yes | âŒ No | Non-minimal, Overfitted |
| Biased | âœ… Yes | âœ… Yes | âŒ No | Non-minimal, Overfitted |

**Redundant columns**: ALL (1,2,3,4,5,6,7,8)

**Analysis**:
- LLM used **every single attribute** to determine age
- This is extreme overfitting - essentially saying "every row is unique"
- In reality, age is NOT functionally determined by measurements (correlation â‰  causation)
- **Verdict**: **False confidence** - holds in data but semantically wrong

---

### 3. BALANCE-SCALE (2 samples, 2 FDs)

**LLM Proposals**:
- Biased sample: `(1,3,4,5) â†’ 2` AND `(2,3,4,5) â†’ 1`
- Random sample: `(2,3,4,5) â†’ 1`

| Sample Type | FD | Holds on Sample | Holds on Full | Classification |
|-------------|-----|-----------------|---------------|----------------|
| Biased | (1,3,4,5) â†’ 2 | âœ… Yes | âŒ **NO** | **FALSE POSITIVE** |
| Biased | (2,3,4,5) â†’ 1 | âœ… Yes | âœ… Yes | âœ… Valid & Minimal |
| Random | (2,3,4,5) â†’ 1 | âœ… Yes | âœ… Yes | âœ… Valid & Minimal |

**Violations on full dataset**: 148 violations for `(1,3,4,5) â†’ 2`

**Analysis**:
- First FD **false positive** - held in biased sample but **breaks on full data**
- Second FD is actually valid and minimal
- **Verdict**: **Sampling bias created spurious FD** (Disagreement #1)

---

### 4. BREAST-CANCER-WISCONSIN (2 samples, 1 FD)

**LLM Proposal**: `(1,2,3,4,5,6,7,8,9,10) â†’ 11` (all 10 features â†’ class)

| Sample Type | Holds on Sample | Holds on Full | Minimal? | Classification |
|-------------|-----------------|---------------|----------|----------------|
| Random | âœ… Yes | âœ… Yes | âŒ No | Non-minimal, Overfitted |
| Biased | âœ… Yes | âœ… Yes | âŒ No | Non-minimal, Overfitted |

**Redundant columns**: ALL (1-10)

**Analysis**:
- Used all 10 cell characteristics to determine cancer class
- Column 1 is likely patient ID (degenerate FD)
- Even without ID, 9 features is excessive overfitting
- **Verdict**: Non-minimal, includes ID-based dependency

---

### 5. BRIDGES (2 samples, 2 FDs)

**LLM Proposals**:
- `1 â†’ 2` (identifier â†’ material)
- `(3,4,5,6,7,8,9,10,11,12) â†’ 13` (10 attributes â†’ one attribute)

| Sample Type | FD | Holds on Sample | Holds on Full | Minimal? | Classification |
|-------------|-----|-----------------|---------------|----------|----------------|
| Random | 1 â†’ 2 | âœ… Yes | âœ… Yes | âœ… Yes | âš ï¸ ID-based (degenerate) |
| Random | (3,4,...,12) â†’ 13 | âœ… Yes | âœ… Yes | âŒ No | Non-minimal, Overfitted |
| Stratified | 1 â†’ 2 | âœ… Yes | âœ… Yes | âœ… Yes | âš ï¸ ID-based (degenerate) |
| Stratified | (3,4,...,12) â†’ 13 | âœ… Yes | âœ… Yes | âŒ No | Non-minimal, Overfitted |

**Redundant columns**: ALL (3-12) in second FD

**Analysis**:
- First FD is ID-based (column 1 = E1, E2, E3... identifier)
- Second FD uses **10 out of 13 columns** - extreme overfitting
- **Verdict**: Detected ID pattern correctly, but overfitted on other FD

---

### 6. CHESS (2 samples, 1 FD)

**LLM Proposal**: `(1,2,3,4,5,6) â†’ 7` (6 out of 7 columns)

| Sample Type | Holds on Sample | Holds on Full | Minimal? | Classification |
|-------------|-----------------|---------------|----------|----------------|
| Random | âœ… Yes | âœ… Yes | âœ… Yes | Overfitted but minimal |
| Stratified | âœ… Yes | âœ… Yes | âœ… Yes | Overfitted but minimal |

**Analysis**:
- Requires 6 out of 7 total attributes
- Actually IS minimal (all 6 needed - matches Task Set 1 finding)
- This suggests the target is computed from all inputs
- **Verdict**: Valid but conceptually overfitted (entire row determines one column)

---

### 7. ECHOCARDIOGRAM (2 samples, 1 FD)

**LLM Proposal**: `(1,2,3,4,5,6,7,8,9,10,11) â†’ 12` (11 out of 12 columns)

| Sample Type | Holds on Sample | Holds on Full | Minimal? | Classification |
|-------------|-----------------|---------------|----------|----------------|
| Random | âœ… Yes | âœ… Yes | âŒ No | Non-minimal, Overfitted |
| Biased | âœ… Yes | âœ… Yes | âŒ No | Non-minimal, Overfitted |

**Redundant columns**: ALL (1-11)

**Analysis**:
- Used 11 out of 12 columns - essentially entire row
- All columns are redundant
- **Verdict**: Extreme overfitting

---

### 8. HEPATITIS (2 samples, 1 FD)

**LLM Proposal**: `(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19) â†’ 20` (19 out of 20 columns!)

| Sample Type | Holds on Sample | Holds on Full | Minimal? | Classification |
|-------------|-----------------|---------------|----------|----------------|
| Random | âŒ **No** (1 violation) | âœ… Yes | âŒ No | Non-minimal, Overfitted |
| Stratified | âŒ **No** (1 violation) | âœ… Yes | âŒ No | Non-minimal, Overfitted |

**Redundant columns**: ALL (1-19)

**Analysis**:
- **Most extreme case**: 19 out of 20 columns!
- Ironically, **violated on the sample itself** (1 violation each)
- Still holds on full dataset (but all attributes redundant)
- **Verdict**: **Absurd overfitting** - LLM generated a "row = row" dependency

---

### 9. NURSERY (2 samples, 1 FD)

**LLM Proposal**: `8 â†’ 9`

| Sample Type | Holds on Sample | Holds on Full | Classification |
|-------------|-----------------|---------------|----------------|
| Random | âŒ No (2 violations) | âŒ **NO** | **FALSE POSITIVE** |
| Stratified | âŒ No (2 violations) | âŒ **NO** | **FALSE POSITIVE** |

**Violations**: 2 on sample, 2 on full dataset
**Example violation**: `('recommended',) â†’ {'priority', 'recommend', 'spec_prior', 'very_recom'}`

**Analysis**:
- **Didn't even hold on the sample!** (2 violations)
- Also false on full dataset
- LLM proposed a FD that was **immediately violated in its training data**
- **Verdict**: **Complete failure** - false positive even on sample (Disagreement #2)

---

## Summary of Issues

### 1. False Positives (3 FDs, 15%)

| Dataset | FD | Issue |
|---------|-----|-------|
| balance-scale | (1,3,4,5) â†’ 2 | Held in biased sample, **148 violations** on full data |
| nursery | 8 â†’ 9 | **Violated even on sample** (2 violations), also false on full data |

**Key Insight**: Sampling hides violations. What holds in 45 rows may break in 12,960 rows.

---

### 2. Non-Minimal FDs (13 FDs, 65%)

**All columns redundant** in these FDs:
- abalone (3 samples): ALL 8 attributes redundant
- breast-cancer-wisconsin (2 samples): ALL 10 attributes redundant
- bridges (2 samples): ALL 10 attributes redundant (in 2nd FD)
- echocardiogram (2 samples): ALL 11 attributes redundant
- hepatitis (2 samples): ALL 19 attributes redundant
- iris (2 samples): ALL 4 attributes redundant

**Pattern**: LLM tends to use **every available attribute** instead of finding minimal sets.

---

### 3. Overfitted FDs (13 FDs, 65%)

FDs with â‰¥6 attributes on LHS:
- hepatitis: 19 attributes (most extreme)
- echocardiogram: 11 attributes
- breast-cancer-wisconsin: 10 attributes
- bridges: 10 attributes
- abalone: 8 attributes
- chess: 6 attributes

**Pattern**: LLM defaults to "use everything" strategy.

---

### 4. Valid & Minimal (4 FDs, 20%)

Only **4 out of 20 FDs** were both valid and minimal:
- balance-scale (random): `(2,3,4,5) â†’ 1` âœ…
- balance-scale (biased): `(2,3,4,5) â†’ 1` âœ…
- bridges (random): `1 â†’ 2` âœ… (but ID-based/degenerate)
- bridges (stratified): `1 â†’ 2` âœ… (but ID-based/degenerate)

**Observation**: Only 2 FDs are truly meaningful and minimal. The bridges FDs are degenerate (ID-based).

---

## Key Insights & Lessons

### 1. **Sampling Hides Violations** âœ… CONFIRMED

**Example**: `balance-scale` biased sample
- FD `(1,3,4,5) â†’ 2` held in 48-row sample
- Violated **148 times** in 625-row full dataset
- **23.7% violation rate** hidden by sampling

**Lesson**: Small samples create false confidence.

---

### 2. **Samples May Reverse Dependencies** âš ï¸ NOT OBSERVED

We didn't observe dependency reversals, but we did see:
- Different samples proposing different FD sets (balance-scale)
- Sample bias affecting which FDs are discovered

**Lesson**: Sampling affects which patterns are visible.

---

### 3. **LLMs Generalize from Tiny Evidence** âœ… CONFIRMED

**Example**: Hepatitis
- Sample: 45 rows
- Full dataset: 155 rows (only 3.4x larger!)
- LLM proposed: `(19 attributes) â†’ 1`
- This is essentially saying "every row is unique"

**Lesson**: LLMs extrapolate aggressively from limited data.

---

### 4. **Empirical Patterns on Samples Are Not Constraints** âœ… CONFIRMED

**Example**: Nursery
- LLM proposed `8 â†’ 9`
- **Violated immediately in the sample itself** (2 violations)
- LLM still proposed it as a "likely FD"

**Lesson**: LLMs don't rigorously validate before proposing.

---

## Critical LLM Limitations Revealed

### 1. **Overfitting Bias** (Most Critical)

**65% of FDs were non-minimal** with ALL attributes redundant.

**Why?** LLMs trained on text lack understanding of:
- Minimality principle
- Occam's Razor for dependencies
- The difference between "sufficient" and "necessary"

**Example**: Instead of finding that `petal_length â†’ species`, LLM proposes `(sepal_length, sepal_width, petal_length, petal_width) â†’ species`

---

### 2. **No Validation Before Proposing**

**Nursery case**:
- Proposed `8 â†’ 9`
- **Already violated in the sample** (2 violations)
- LLM didn't check its own training data

**Why?** LLMs generate plausible-sounding FDs based on:
- Column name semantics
- Statistical patterns
- But NOT rigorous data validation

---

### 3. **Lack of Statistical Rigor**

**Hepatitis case**:
- 45-row sample
- Proposed 19-attribute dependency
- Statistical impossibility to validate with so few rows

**Why?** LLMs don't understand sample size requirements for validation.

---

## Comparison: LLM vs. Algorithmic FD Discovery

| Aspect | LLM (from samples) | TANE (from full data) |
|--------|-------------------|----------------------|
| **Minimality** | 20% minimal | ~80% minimal (by design) |
| **False Positives** | 15% | 0% (by definition) |
| **Overfitting** | 65% overfitted | Varies, but detectable |
| **Validation** | No checking | Exhaustive checking |
| **Semantic awareness** | High | None |
| **Speed** | Fast | Slow (exponential) |

**Conclusion**: LLMs are **fast but sloppy**. Algorithms are **slow but precise**.

---

## Answers to Task Set 3 Questions

### Q1: Which FDs are false positives?

**3 false positives (15%)**:

1. **balance-scale**: `(1,3,4,5) â†’ 2`
   - Held in biased sample (48 rows)
   - **148 violations** in full dataset (625 rows)
   - 23.7% violation rate

2. **nursery**: `8 â†’ 9`
   - Violated even in sample (2 violations)
   - Also false on full dataset
   - LLM proposed despite immediate evidence against it

---

### Q2: Which are not minimal?

**13 non-minimal FDs (65%)**:

All FDs from:
- abalone (all 3 samples)
- breast-cancer-wisconsin (both samples)
- bridges (2nd FD in both samples)
- echocardiogram (both samples)
- hepatitis (both samples)
- iris (both samples)

**Pattern**: In most cases, **ALL attributes on LHS were redundant**.

---

### Q3: Which are misleading but "look right"?

**Several categories**:

1. **Semantically correct but overfitted**:
   - iris: `(1,2,3,4) â†’ 5` (measurements â†’ species)
   - Relationship is real, but uses too many attributes

2. **ID-based (trivial)**:
   - bridges: `1 â†’ 2` (identifier â†’ material)
   - Valid in data but provides no domain knowledge

3. **Correlation disguised as causation**:
   - abalone: `(all measurements) â†’ age`
   - Age correlates with size, but doesn't functionally determine it
   - LLM accepted correlation as functional dependency (Task Set 2 insight)

---

## Recommendations

### For Using LLMs in FD Discovery:

1. **Use as hypothesis generator only** âœ…
   - LLMs are good at suggesting plausible relationships
   - But must validate with algorithmic checking

2. **Never trust without validation** âŒ
   - 15% false positive rate is too high
   - 65% overfitting rate is unacceptable

3. **Apply minimality filtering** âš ï¸
   - Automatically test if removing attributes preserves FD
   - Reduce overfitting post-hoc

4. **Require larger samples** ðŸ“Š
   - 45 rows is insufficient for 19-attribute dependencies
   - Need statistical power calculations

5. **Hybrid approach** ðŸ”„
   - LLM: semantic screening + hypothesis generation
   - Algorithm: rigorous validation + minimization
   - Human: final plausibility check

---

## Conclusion

**Task Set 3 validates the assignment's core insight**:

> **"Sampling creates hypotheses, not truth."**

**What we learned**:

1. âœ… **Sampling hides violations**: 15% false positive rate
2. âœ… **LLMs overfit aggressively**: 65% non-minimal with ALL attributes redundant
3. âœ… **LLMs don't validate**: Proposed FDs that violate their own training data
4. âœ… **Empirical patterns â‰  constraints**: What holds in 45 rows breaks in 12,960

**The danger**: LLM-proposed FDs **look plausible** but are **statistically unreliable**.

**The solution**: **Always validate** LLM hypotheses with algorithmic FD checking on full datasets.

---

## Files Generated

1. **parse_llm_fds.py**: Extracted FDs from Excel file
2. **validate_llm_fds.py**: Automated validation on samples and full datasets
3. **TASK_SET_3_REPORT.md**: This comprehensive analysis

---

**End of Task Set 3 Report**
